{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_qlpNrv1knPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook illustrates the process of finetuning BERT for tech product names Named Entity Recognition (NER) by generating training data with GPT-4 (accessed via OpenAI API)"
      ],
      "metadata": {
        "id": "sK6yjDcWXkCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initial dataset you need is a list of product names. For effective finetuning, you should aim for at least a few thousand data points."
      ],
      "metadata": {
        "id": "fA5DDnsCX-TT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sv3NnbYmXU6D"
      },
      "outputs": [],
      "source": [
        "# sample_initial dataset = ['MacBook Pro',  'DIZO Star 500', 'Asus ZenBook UX430UN', 'Acer Aspire 3', etc]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now generate training data with the initial dataset by using GPT-4 though Langchain and OpenAI API."
      ],
      "metadata": {
        "id": "mpQxbKgWY3JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import Langchain & OpenAI to set up\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "9xPVL8W3YZ-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up LLM. set temperature to 0.9 to prevent generating crazy sentences.\n",
        "chat_llm = ChatOpenAI(openai_api_key = 'YOUR OPENAI API KEY HERE', model_name='gpt-4', temperature=0.9)"
      ],
      "metadata": {
        "id": "nHDZHWiqYzlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write prompt templates to be passed to GPT-4. more templates allows generating more diverse training data. I give 3 examples here but you should add more.\n",
        "# very important to specify the exact number of sentences to generate. 68 is just a number that works for my dataset; you can change it. separate each sentence with a separator so that the sentences can be easily split later.\n",
        "template1 = '''\n",
        "you are a customer complaining about the product you bought being lost or damanged. write a 1 sentence complaint about each of the following 68 products. you will generate 68 sentences. separate each sentence with #. insert # after the ending punctuation of the sentence.\n",
        "\n",
        "products:\n",
        "{product_names}\n",
        "'''\n",
        "\n",
        "template2 = '''\n",
        "you are a tech reviewer. write 1 sentence about each of the following 68 products. you will generate 68 sentences. separate each sentence with #. insert # after the ending punctuation of the sentence.\n",
        "\n",
        "products:\n",
        "{product_names}\n",
        "'''\n",
        "\n",
        "template3 = '''\n",
        "you are a customer shopping for laptops and mobile phones. you are talking to a customer support agent in an online chat. write a 1 sentence inquiry about each of the following 68 products. you will generate 68 sentences. separate each sentence with #. insert # after the ending punctuation of the sentence.\n",
        "\n",
        "products:\n",
        "{product_names}\n",
        "'''\n"
      ],
      "metadata": {
        "id": "2a5m21OXZdxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function dividing initial dataset into smaller chunks to passed through OpenAI API. The API can't hand too much data at once.\n",
        "def split_initial_dataset(initial_dataset, num_chunks):\n",
        "    avg_chunk_size = len(initial_dataset) // num_chunks\n",
        "    chunks = []\n",
        "    for i in range(num_chunks):\n",
        "        start_index = i * avg_chunk_size + min(i, len(initial_dataset) % num_chunks)\n",
        "        end_index = start_index + avg_chunk_size + (1 if i < len(initial_dataset) % num_chunks else 0)\n",
        "        chunks.append(initial_dataset.iloc[start_index:end_index])\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "2jqwNrXobca6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for generating training data by asking GPT-4 to write a sentence for each product name in the list. for each list, the function ramdonly chooses a prompt template to increase the variety of the training data\n",
        "templates_list = [template1, template2, template3]\n",
        "def generate_responses(product_names_list):\n",
        "    product_names = product_names_list\n",
        "    prompt = PromptTemplate(template = random.choice(templates_list), input_variables=['product_names'])\n",
        "    llm_chain = LLMChain(prompt = prompt, llm = chat_llm, verbose= True)\n",
        "    response = llm_chain.run({'product_names': product_names})\n",
        "    return response"
      ],
      "metadata": {
        "id": "FEKxw3rPaf62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate training data 3 chunks at a time. you can change the number, but keep it small since the API might malfunction if you pass too many at a time.\n",
        "llm_response_list = []\n",
        "for i in range(0, len(chunks), 3):\n",
        "    chunk = chunks[i:i+3]\n",
        "    for product_names_list in chunk:\n",
        "        response = generate_responses(product_names_list)\n",
        "        llm_response_list.append(response)"
      ],
      "metadata": {
        "id": "nW7YF42QakAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for spliting the LLM response into individiual sentences\n",
        "def split_string(input_string):\n",
        "    split_list = input_string.split('#')\n",
        "    split_list = [sentence.strip() for sentence in split_list if sentence.strip()]\n",
        "    return split_list"
      ],
      "metadata": {
        "id": "mmIOzqPgamtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# append generated sentences to a list. this is the generated training data.\n",
        "sentences = []\n",
        "for i in llm_response_list:\n",
        "    split_sentences_list = split_string(i)\n",
        "    sentences.extend(split_sentences_list)"
      ],
      "metadata": {
        "id": "oArxvicOawr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the training data generation was done correctly, you now should have two lists of equal length: a product name list (same as the initial dataset list) and a sentence list."
      ],
      "metadata": {
        "id": "FWedEu1ueE2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up nltk tokenizer\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "255Yy1zPen7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for finding the indexes of the product name keywords in the sentence. supports passing in a single pair of sentence and product name\n",
        "def find_keyword_indexes(sentence, product_name):\n",
        "    # Tokenizing the sentence and keywords with nltk tokenizer\n",
        "    words = word_tokenize(sentence)\n",
        "    keyword_words = word_tokenize(product_name)\n",
        "    words = [word.lower() for word in words]\n",
        "    keyword_words = [word.lower() for word in keyword_words]\n",
        "\n",
        "    keyword_indexes = []\n",
        "\n",
        "    i = 0\n",
        "    while i <= len(words) - len(keyword_words):\n",
        "        if words[i:i+len(keyword_words)] == keyword_words:\n",
        "            keyword_indexes.extend(range(i, i+len(keyword_words)))\n",
        "        i += 1\n",
        "\n",
        "    return keyword_indexes"
      ],
      "metadata": {
        "id": "YYd7SWjwdtN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for finding the indexes of the product name keywords in the sentence. supports passing in lists of sentences and product names. returns a zipped list\n",
        "def find_keywords_indexes(sentences_list, product_name_list):\n",
        "    indexes_list = []\n",
        "    for sentence, keywords in list(zip(sentences_list, product_name_list)):\n",
        "        indexes = find_keyword_indexes(sentence, keywords)\n",
        "        indexes_list.append(indexes)\n",
        "    return list(zip(sentences_list, indexes_list))"
      ],
      "metadata": {
        "id": "wpyjKaGnfwrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pass the zipped list to this function to generate NER tags for each token\n",
        "def generate_tags(zipped_list):\n",
        "    word_labels_list = []\n",
        "    for sentence, indexes in zipped_list:\n",
        "        labels_list = []\n",
        "        tokens = word_tokenize(sentence)\n",
        "        for token in tokens:\n",
        "            index = tokens.index(token)\n",
        "            if index == indexes[0]:\n",
        "                labels_list.append('B-pn')\n",
        "            elif index in indexes and index != indexes[0]:\n",
        "                labels_list.append('I-pn')\n",
        "            else:\n",
        "                labels_list.append('O')\n",
        "        labels = ', '.join(labels_list)\n",
        "        word_labels_list.append(labels)\n",
        "    return word_labels_list"
      ],
      "metadata": {
        "id": "GizkU5z-giAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for converting lists into a pandas dataframe\n",
        "import pandas as pd\n",
        "def lists_to_dataframe(list1, list2, column_names=['Column1', 'Column2']):\n",
        "    if len(list1) != len(list2):\n",
        "        raise ValueError(\"Input lists must have the same length.\")\n",
        "    data = {column_names[0]: list1, column_names[1]: list2}\n",
        "    df = pd.DataFrame(data)\n",
        "    return df"
      ],
      "metadata": {
        "id": "FQH8FxJig1mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dataframe that contains a column of sentences and a column of the corresponding sentences' NER tags\n",
        "data = lists_to_dataframe(sentences_list, word_labels_list, column_names=['sentence', 'word_labels'])"
      ],
      "metadata": {
        "id": "qAPPNIk8hFUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove spaces in the word_labels column\n",
        "data['word_labels'] = data['word_labels'].str.replace(' ', '')"
      ],
      "metadata": {
        "id": "WZ7tagVDhdx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assign NER tags numerical ids\n",
        "label2id = {'I-pn': 2, 'B-pn': 1, 'O': 0}\n",
        "id2label = {0 : 'O', 1 : 'B-pn', 2: 'I-pn'}"
      ],
      "metadata": {
        "id": "gpUlhQfQhztR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries for training\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertConfig, BertForTokenClassification"
      ],
      "metadata": {
        "id": "iUtEgB8Hh_-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check cuda\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "id": "JasJJd_EiO2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure training\n",
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 2\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "pjJgBh6hiRCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up BERT tokenizer\n",
        "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
        "    \"\"\"\n",
        "    Word piece tokenization makes it difficult to match word labels\n",
        "    back up with individual word pieces. This function tokenizes each\n",
        "    word one at a time so that it is easier to preserve the correct\n",
        "    label for each subword. It is, of course, a bit slower in processing\n",
        "    time, but it will help our model achieve higher accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
        "\n",
        "        # Tokenize the word and count # of subwords the word is broken into\n",
        "        tokenized_word = tokenizer.tokenize(word)\n",
        "        n_subwords = len(tokenized_word)\n",
        "\n",
        "        # Add the tokenized word to the final tokenized word list\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "        # Add the same label to the new list of labels `n_subwords` times\n",
        "        labels.extend([label] * n_subwords)\n",
        "\n",
        "    return tokenized_sentence, labels"
      ],
      "metadata": {
        "id": "AXJZFCULidsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define dataset class to pre-process dataset for training\n",
        "class dataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # step 1: tokenize (and adapt corresponding labels)\n",
        "        sentence = self.data.sentence[index]\n",
        "        word_labels = self.data.word_labels[index]\n",
        "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
        "\n",
        "        # step 2: add special tokens (and corresponding labels)\n",
        "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
        "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
        "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
        "\n",
        "        # step 3: truncating/padding\n",
        "        maxlen = self.max_len\n",
        "\n",
        "        if (len(tokenized_sentence) > maxlen):\n",
        "          # truncate\n",
        "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
        "          labels = labels[:maxlen]\n",
        "        else:\n",
        "          # pad\n",
        "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
        "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
        "\n",
        "        # step 4: obtain the attention mask\n",
        "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
        "\n",
        "        # step 5: convert tokens to input ids\n",
        "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
        "\n",
        "        label_ids = [label2id[label] for label in labels]\n",
        "        # the following line is deprecated\n",
        "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
        "\n",
        "        return {\n",
        "              'ids': torch.tensor(ids, dtype=torch.long),\n",
        "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
        "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
        "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "h_CLqDdkieQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset into training and testing\n",
        "train_size = 0.8\n",
        "train_dataset = data.sample(frac=train_size,random_state=200)\n",
        "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(data.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "F3-lS2TLiiqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up params and dataloaders\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "metadata": {
        "id": "GsJ23wMXil54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load BERT model\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased',\n",
        "                                                   num_labels=len(id2label),\n",
        "                                                   id2label=id2label,\n",
        "                                                   label2id=label2id)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "CnasqI0oin-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify initial loss\n",
        "ids = training_set[0][\"ids\"].unsqueeze(0)\n",
        "mask = training_set[0][\"mask\"].unsqueeze(0)\n",
        "targets = training_set[0][\"targets\"].unsqueeze(0)\n",
        "ids = ids.to(device)\n",
        "mask = mask.to(device)\n",
        "targets = targets.to(device)\n",
        "outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "initial_loss = outputs[0]\n",
        "initial_loss"
      ],
      "metadata": {
        "id": "Uh9mIohLiol0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify tensor shape\n",
        "tr_logits = outputs[1]\n",
        "tr_logits.shape"
      ],
      "metadata": {
        "id": "cu8dIhktitbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up optimizer\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "8fbi9Rx6iu7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start training\n",
        "def train(epoch):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    # put model in training mode\n",
        "    model.train()\n",
        "\n",
        "    for idx, batch in enumerate(training_loader):\n",
        "\n",
        "        ids = batch['ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['mask'].to(device, dtype = torch.long)\n",
        "        targets = batch['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "        loss, tr_logits = outputs.loss, outputs.logits\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += targets.size(0)\n",
        "\n",
        "        if idx % 100==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "\n",
        "        # compute training accuracy\n",
        "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
        "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "        tr_preds.extend(predictions)\n",
        "        tr_labels.extend(targets)\n",
        "\n",
        "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "        )\n",
        "\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f\"Training epoch: {epoch + 1}\")\n",
        "  train(epoch)"
      ],
      "metadata": {
        "id": "AjtY1u9QixCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate trained model performance\n",
        "def valid(model, testing_loader):\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "\n",
        "            ids = batch['ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['mask'].to(device, dtype = torch.long)\n",
        "            targets = batch['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "            loss, eval_logits = outputs.loss, outputs.logits\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += targets.size(0)\n",
        "\n",
        "            if idx % 100==0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "\n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
        "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "            eval_labels.extend(targets)\n",
        "            eval_preds.extend(predictions)\n",
        "\n",
        "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    #print(eval_labels)\n",
        "    #print(eval_preds)\n",
        "\n",
        "    labels = [id2label[id.item()] for id in eval_labels]\n",
        "    predictions = [id2label[id.item()] for id in eval_preds]\n",
        "\n",
        "    #print(labels)\n",
        "    #print(predictions)\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return labels, predictions\n",
        "\n",
        "labels, predictions = valid(model, testing_loader)"
      ],
      "metadata": {
        "id": "xq5mZVagiy_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check precision, recall, and f1-score\n",
        "from seqeval.metrics import classification_report\n",
        "\n",
        "print(classification_report([labels], [predictions]))"
      ],
      "metadata": {
        "id": "gMLukRz0i4O_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}